{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4524eb1b",
   "metadata": {},
   "source": [
    "# SEC 13F-Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ce408",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4ba1a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5da56f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from datetime import datetime\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f35b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selector(driver, by, value, timeout=5):\n",
    "    return WebDriverWait(driver, timeout).until(\n",
    "        EC.presence_of_element_located((by, value))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quarter(date):\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "    if month in [4, 5, 6]:\n",
    "        return \"q1\", year\n",
    "    elif month in [7, 8, 9]:\n",
    "        return \"q2\", year\n",
    "    elif month in [10, 11, 12]:\n",
    "        return \"q3\", year\n",
    "    elif month in [1, 2, 3]:\n",
    "        return \"q4\", year - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc65d4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_share(scraped_count, total_count):\n",
    "    if total_count == 0:\n",
    "        return 0\n",
    "    share = (scraped_count / total_count) * 100\n",
    "    return round(share)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "645f4c38-6d5c-46a3-a7b4-75103a0c6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(cik, data, today_ignore='no'):\n",
    "    driver = webdriver.Chrome()\n",
    "    os.makedirs('data_sec', exist_ok=True)\n",
    "\n",
    "    # Get current date and calculate quarter and adjusted year\n",
    "    date_current = datetime.now()\n",
    "    date_str = date_current.strftime(\"%Y-%m-%d\")\n",
    "    quarter, year = get_quarter(date_current)  # Use renamed function here\n",
    "    \n",
    "    # Initialize counters for share calculation\n",
    "    entries_total = len(data)  # Total entries in the dataset\n",
    "    entries_scraped = 0  # Entries successfully processed (scraped or skipped due to today's date)\n",
    "\n",
    "    # Placeholder for temporary CSV file path (only created when valid entries are scraped)\n",
    "    path_csv_temp = None\n",
    "\n",
    "    filed_updated = []\n",
    "    entries_error = []  # To store rows where errors occurred\n",
    "\n",
    "    for id in range(len(data)):\n",
    "        cik_value = str(data.loc[id, 'cik']).zfill(10)\n",
    "        filed_old = data.loc[id, 'filed']\n",
    "\n",
    "        # Skip rows with today's date if today_ignore is 'yes'\n",
    "        if today_ignore.lower() == 'yes' and filed_old == date_str:\n",
    "            print(f\"CIK {cik_value}: Entry has today's date ({filed_old}). Skipping ...\")\n",
    "            filed_updated.append(filed_old)  # Keep the old filed value\n",
    "            entries_scraped += 1  # Increment the counter since it's considered processed\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            id_value = data.loc[id, 'id']\n",
    "            vip_value = data.loc[id, 'vip']\n",
    "            name_value = data.loc[id, 'name']\n",
    "\n",
    "            url = f\"https://www.sec.gov/edgar/search/#/q={cik_value}&filter_forms=13F-HR&sort=desc\"\n",
    "            driver.get(url)\n",
    "            \n",
    "            time.sleep(2)  # Short wait for initial load\n",
    "\n",
    "            try:\n",
    "                result_first = selector(driver, By.CSS_SELECTOR, \"a.preview-file[data-adsh]\")\n",
    "            except TimeoutException:\n",
    "                print(f\"CIK {cik_value}: No search results found. Skipping ...\")\n",
    "                filed_updated.append(filed_old)  # Keep the old filed value if no results are found\n",
    "                entries_error.append({'id': id_value, 'vip': vip_value, 'name': name_value, 'cik': cik_value})\n",
    "                continue\n",
    "\n",
    "            filed_new = selector(driver, By.XPATH, \"/html/body/div[3]/div[2]/div[2]/table/tbody/tr[1]/td[2]\").text\n",
    "            reported_new = selector(driver, By.XPATH, \"/html/body/div[3]/div[2]/div[2]/table/tbody/tr[1]/td[3]\").text\n",
    "\n",
    "            # Compare filed dates\n",
    "            if pd.notna(filed_old) and filed_old != '':\n",
    "                date_old = datetime.strptime(filed_old, \"%Y-%m-%d\")\n",
    "                date_new = datetime.strptime(filed_new, \"%Y-%m-%d\")\n",
    "                if date_new <= date_old:\n",
    "                    print(f\"CIK {cik_value}: No new filing. Skipping ...\")\n",
    "                    filed_updated.append(filed_old)\n",
    "                    entries_scraped += 1  # Increment the counter since it's considered processed\n",
    "                    continue  # Skip to the next CIK\n",
    "\n",
    "            print(f\"CIK {cik_value}: New filing found. Scraping ...\")\n",
    "            filed_updated.append(filed_new)\n",
    "\n",
    "            result_first.click()\n",
    "\n",
    "            open_filing_button = selector(driver, By.XPATH, \"//button[contains(@class, 'btn-warning') and contains(text(), 'Open filing')]\")\n",
    "            open_filing_button.click()\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            # Implement the speedup method\n",
    "            driver.set_page_load_timeout(3)\n",
    "            try:\n",
    "                driver.get(driver.current_url)\n",
    "            except TimeoutException:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "\n",
    "            target_link_xpath = \"//table[@class='tableFile']/tbody/tr[4]/td[3]/a\"\n",
    "            target_link = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, target_link_xpath))\n",
    "            )\n",
    "            target_link.click()\n",
    "\n",
    "            table = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"/html/body/table[2]\"))\n",
    "            )\n",
    "            \n",
    "            table_html = table.get_attribute('outerHTML')\n",
    "            \n",
    "            # Use StringIO to wrap the HTML string\n",
    "            html_io = StringIO(table_html)\n",
    "            df = pd.read_html(html_io)[0]\n",
    "\n",
    "            df.columns = [\n",
    "                \"company\", \"stock type\", \"cusip-id\", \"figi-id\", \"value\", \n",
    "                \"number\", \"principal amount\", \"call/put\", \"discretion\", \n",
    "                \"manager\", \"sole voting authority\", \"shared voting authority\", \n",
    "                \"none voting authority\"\n",
    "            ]\n",
    "\n",
    "            df_input = pd.DataFrame({\n",
    "                'id': id_value,\n",
    "                'vip': vip_value,\n",
    "                'name': name_value,\n",
    "                'cik': cik_value,\n",
    "                'filed': filed_new,\n",
    "                'enddate': reported_new\n",
    "            }, index=df.index)\n",
    "\n",
    "            df = pd.concat([df_input, df], axis=1)\n",
    "\n",
    "            # Create temporary file path only when valid entries are scraped\n",
    "            if path_csv_temp is None:\n",
    "                path_csv_temp = os.path.join('data_sec', f'{year}-{quarter}_temp_{date_str}.csv')\n",
    "\n",
    "            df.to_csv(path_csv_temp, mode='a', header=not os.path.exists(path_csv_temp), index=False)\n",
    "\n",
    "            # Increment scraped entries count\n",
    "            entries_scraped += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CIK {cik_value}: An error occurred! {e}\")\n",
    "            \n",
    "            # Append details of the entry with an error to entries_error list\n",
    "            id_value = data.loc[id, 'id']\n",
    "            vip_value = data.loc[id, 'vip']\n",
    "            name_value = data.loc[id, 'name']\n",
    "            \n",
    "            entries_error.append({'id': id_value, 'vip': vip_value, 'name': name_value, 'cik': cik_value})\n",
    "            \n",
    "            filed_updated.append(filed_old)  # Keep the existing value in case of error\n",
    "\n",
    "        finally:\n",
    "            # Close all tabs except the first one\n",
    "            while len(driver.window_handles) > 1:\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    if entries_scraped > 0 and path_csv_temp is not None:\n",
    "        # Calculate share and rename file with share percentage\n",
    "        share = calculate_share(entries_scraped, entries_total)\n",
    "        path_csv_finalized = os.path.join('data_sec', f'{year}-{quarter}_{share}%_{date_str}.csv')\n",
    "        os.rename(path_csv_temp, path_csv_finalized)\n",
    "        print(f\"Data saved to {path_csv_finalized}\")\n",
    "    else:\n",
    "        print(\"No entries scraped. No file created.\")\n",
    "\n",
    "    # Save error entries to a separate CSV file if any errors occurred\n",
    "    if entries_error:\n",
    "        complement_share = round(100 - calculate_share(entries_scraped, entries_total))  # Complement of share (as percentage)\n",
    "        error_file_path = os.path.join('data_sec', f'error_{year}-{quarter}_{complement_share}%_{date_str}.csv')\n",
    "        pd.DataFrame(entries_error).to_csv(error_file_path, index=False)\n",
    "        print(f\"Error details saved to {error_file_path}\")\n",
    "\n",
    "    # Update test.csv with new filed values only after all scraping is done\n",
    "    data['filed'] = filed_updated\n",
    "    data.to_csv('data/investors_cik.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee6f31e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIK 0001336528: No new filing. Skipping ...\n",
      "CIK 0001067983: No new filing. Skipping ...\n",
      "CIK 0001096343: No new filing. Skipping ...\n",
      "CIK 0000859804: No new filing. Skipping ...\n",
      "CIK 0000807985: No new filing. Skipping ...\n",
      "CIK 0001035674: No new filing. Skipping ...\n",
      "CIK 0000915191: No new filing. Skipping ...\n",
      "CIK 0000883965: New filing found. Scraping ...\n",
      "Data saved to data_sec/2024-q4_100%_2025-01-07.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    data = pd.read_csv('data/investors_cik.csv')\n",
    "    df_cik = data['cik'].astype(str).tolist()\n",
    "    \n",
    "    today_ignore = 'yes'\n",
    "    \n",
    "    scraper(df_cik,data,today_ignore)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf229f",
   "metadata": {},
   "source": [
    "- Berechnung Quartalsabhängig machen\n",
    "- zweiten Scraper für zusatzinfos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f16544",
   "metadata": {},
   "source": [
    "This is the code that checks for last quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca24d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(cik, data):\n",
    "    driver = webdriver.Chrome()\n",
    "    os.makedirs('data_sec', exist_ok=True)\n",
    "\n",
    "    current_date = datetime.now()\n",
    "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
    "    quarter, year = quarter(current_date)\n",
    "    \n",
    "    path_csv = os.path.join('data_sec', f'{year}-{quarter}_{date_str}.csv')\n",
    "\n",
    "    filed_updated = []\n",
    "\n",
    "    for id, cik in enumerate(cik):\n",
    "        try:\n",
    "            cik_value = str(cik).zfill(10)\n",
    "            id_value = data.loc[id, 'id']\n",
    "            vip_value = data.loc[id, 'vip']\n",
    "            name_value = data.loc[id, 'name']\n",
    "            filed_old = data.loc[id, 'filed']\n",
    "\n",
    "            url = f\"https://www.sec.gov/edgar/search/#/q={cik_value}&filter_forms=13F-HR&sort=desc\"\n",
    "            driver.get(url)\n",
    "            \n",
    "            time.sleep(2)  # Short wait for initial load\n",
    "\n",
    "            result_first = selector(driver, By.CSS_SELECTOR, \"a.preview-file[data-adsh]\")\n",
    "\n",
    "            filed_new = selector(driver, By.XPATH, \"/html/body/div[3]/div[2]/div[2]/table/tbody/tr[1]/td[2]\").text\n",
    "            quarter_new = selector(driver, By.XPATH, \"/html/body/div[3]/div[2]/div[2]/table/tbody/tr[1]/td[3]\").text\n",
    "\n",
    "            # Compare filed dates\n",
    "            if pd.notna(filed_old) and filed_old != '':\n",
    "                # Parse old and new filed dates\n",
    "                date_old = datetime.strptime(filed_old, \"%Y-%m-%d\")\n",
    "                date_new = datetime.strptime(filed_new, \"%Y-%m-%d\")\n",
    "\n",
    "                # Determine current quarter and year\n",
    "                date_current = datetime.now()\n",
    "                quarter_current, year_current = quarter(date_current)\n",
    "\n",
    "                # Determine quarter and year of date_old\n",
    "                quarter_old, year_old = quarter(date_old)\n",
    "\n",
    "                # Skip if date_old is from a previous quarter\n",
    "                if (year_old < year_current or quarter_old != quarter_current):\n",
    "                    print(f\"CIK {cik_value}: Filing is from a previous quarter ({quarter_old}, {year_old}). Skipping...\")\n",
    "                    filed_updated.append(filed_old)\n",
    "                    continue  # Skip to the next CIK\n",
    "\n",
    "                # Skip if date_new is not newer than date_old\n",
    "                if date_new <= date_old:\n",
    "                    print(f\"CIK {cik_value}: No new filing in the current quarter. Skipping...\")\n",
    "                    filed_updated.append(filed_old)\n",
    "                    continue  # Skip to the next CIK\n",
    "\n",
    "            print(f\"New filing found for CIK {cik_value}. Proceeding with scraping...\")\n",
    "            filed_updated.append(filed_new)\n",
    "\n",
    "            result_first.click()\n",
    "\n",
    "            open_filing_button = selector(driver, By.XPATH, \"//button[contains(@class, 'btn-warning') and contains(text(), 'Open filing')]\")\n",
    "            open_filing_button.click()\n",
    "\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "\n",
    "            driver.set_page_load_timeout(3)\n",
    "            try:\n",
    "                driver.get(driver.current_url)\n",
    "            except TimeoutException:\n",
    "                driver.execute_script(\"window.stop();\")\n",
    "\n",
    "            target_link_xpath = \"//table[@class='tableFile']/tbody/tr[4]/td[3]/a\"\n",
    "            target_link = WebDriverWait(driver, 5).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, target_link_xpath))\n",
    "            )\n",
    "            target_link.click()\n",
    "\n",
    "            table = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"/html/body/table[2]\"))\n",
    "            )\n",
    "            \n",
    "            table_html = table.get_attribute('outerHTML')\n",
    "            \n",
    "            html_io = StringIO(table_html)\n",
    "            df = pd.read_html(html_io)[0]\n",
    "\n",
    "            df.columns = [\n",
    "                \"company\", \"stock type\", \"cusip-id\", \"figi-id\", \"value\", \n",
    "                \"number\", \"principal amount\", \"call/put\", \"discretion\", \n",
    "                \"manager\", \"sole voting authority\", \"shared voting authority\", \n",
    "                \"none voting authority\"\n",
    "            ]\n",
    "\n",
    "            df_input = pd.DataFrame({\n",
    "                'id': id_value,\n",
    "                'vip': vip_value,\n",
    "                'name': name_value,\n",
    "                'cik': cik_value,\n",
    "                'filed': filed_new,\n",
    "                'enddate': quarter_new\n",
    "            }, index=df.index)\n",
    "\n",
    "            df.to_csv(path_csv, mode='a', header=not os.path.exists(path_csv), index=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for CIK {cik_value}: {e}\")\n",
    "            filed_updated.append(filed_old)  # Keep existing value in case of error\n",
    "\n",
    "        finally:\n",
    "            while len(driver.window_handles) > 1:\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                driver.close()\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    data['filed'] = filed_updated\n",
    "    data.to_csv('data/test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af06eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "1\tBill Ackman\tPershing Square Capital Management\t1336528\n",
    "2\tWarren Buffett\tBerkshire Hathaway\t1067983\n",
    "3\tTom Gayner\tMarkel Corporation\t1096343\n",
    "4\tTony Guerrerio and David Rolfe\tWedgewood Partners\t859804\n",
    "5\tMason Hawkins\tSoutheastern Asset Management\t807985\n",
    "6\tJohn Paulson\tPaulson & Co.\t1035674\n",
    "7\tPrem Watsa\tFairfax Financial Holdings\t915191\n",
    "8\tWally Weitz\tWallace R. Weitz & Company\t883965\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd47a2d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
